{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "import torch.utils.data \n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms as transforms \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "epochs = 10 \n",
    "train_batch_size = 10\n",
    "test_batch_size = 10\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1dbdba1a223468f8909aa1b3702ff4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df85fdc5571a4a2388493a2b895087c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b131b71bb1f43989b7af434d2555158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82bc42452ae48e59366a2bde7dd8b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.ToTensor()])\n",
    "test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform = train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_set, batch_size = train_batch_size, shuffle=True)\n",
    "test_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform = test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_set, batch_size = test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available:\n",
    "    device = torch.device('cuda:0')\n",
    "    print('Using GPU')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1= nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2), \n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2=nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2), \n",
    "            nn.BatchNorm2d(32), \n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc1=nn.Linear(7*7*32, 10)\n",
    "#         self.fc2=nn.Linear(1024,512)\n",
    "#         self.fc3=nn.Linear(512,10)\n",
    "    def forward(self, x):\n",
    "        out=self.layer1(x)\n",
    "        out=self.layer2(out)\n",
    "        out=out.reshape(out.size(0), -1)\n",
    "#         out=F.relu(self.fc1(out))\n",
    "#         out=F.relu(self.fc2(out))\n",
    "#         out=F.softmax(self.fc3(out))\n",
    "        out=self.fc1(out)\n",
    "        return out\n",
    "model = ConvNet(10).to(device)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),  lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], step [100,6000], Loss: 0.3927\n",
      "Epoch [1/10], step [200,6000], Loss: 0.2861\n",
      "Epoch [1/10], step [300,6000], Loss: 0.0674\n",
      "Epoch [1/10], step [400,6000], Loss: 0.0932\n",
      "Epoch [1/10], step [500,6000], Loss: 0.0305\n",
      "Epoch [1/10], step [600,6000], Loss: 0.0178\n",
      "Epoch [1/10], step [700,6000], Loss: 0.0104\n",
      "Epoch [1/10], step [800,6000], Loss: 0.4332\n",
      "Epoch [1/10], step [900,6000], Loss: 0.0509\n",
      "Epoch [1/10], step [1000,6000], Loss: 0.0198\n",
      "Epoch [1/10], step [1100,6000], Loss: 0.0278\n",
      "Epoch [1/10], step [1200,6000], Loss: 0.5175\n",
      "Epoch [1/10], step [1300,6000], Loss: 0.1561\n",
      "Epoch [1/10], step [1400,6000], Loss: 0.0164\n",
      "Epoch [1/10], step [1500,6000], Loss: 0.0247\n",
      "Epoch [1/10], step [1600,6000], Loss: 0.6712\n",
      "Epoch [1/10], step [1700,6000], Loss: 0.0274\n",
      "Epoch [1/10], step [1800,6000], Loss: 0.1233\n",
      "Epoch [1/10], step [1900,6000], Loss: 0.2908\n",
      "Epoch [1/10], step [2000,6000], Loss: 0.0270\n",
      "Epoch [1/10], step [2100,6000], Loss: 0.5088\n",
      "Epoch [1/10], step [2200,6000], Loss: 0.2312\n",
      "Epoch [1/10], step [2300,6000], Loss: 0.0351\n",
      "Epoch [1/10], step [2400,6000], Loss: 0.0020\n",
      "Epoch [1/10], step [2500,6000], Loss: 1.0623\n",
      "Epoch [1/10], step [2600,6000], Loss: 0.0078\n",
      "Epoch [1/10], step [2700,6000], Loss: 0.0083\n",
      "Epoch [1/10], step [2800,6000], Loss: 0.0957\n",
      "Epoch [1/10], step [2900,6000], Loss: 0.0231\n",
      "Epoch [1/10], step [3000,6000], Loss: 0.0324\n",
      "Epoch [1/10], step [3100,6000], Loss: 0.2294\n",
      "Epoch [1/10], step [3200,6000], Loss: 0.0043\n",
      "Epoch [1/10], step [3300,6000], Loss: 0.0028\n",
      "Epoch [1/10], step [3400,6000], Loss: 0.0014\n",
      "Epoch [1/10], step [3500,6000], Loss: 0.1047\n",
      "Epoch [1/10], step [3600,6000], Loss: 0.0408\n",
      "Epoch [1/10], step [3700,6000], Loss: 0.0221\n",
      "Epoch [1/10], step [3800,6000], Loss: 0.0092\n",
      "Epoch [1/10], step [3900,6000], Loss: 0.7528\n",
      "Epoch [1/10], step [4000,6000], Loss: 0.0154\n",
      "Epoch [1/10], step [4100,6000], Loss: 0.1032\n",
      "Epoch [1/10], step [4200,6000], Loss: 0.0535\n",
      "Epoch [1/10], step [4300,6000], Loss: 0.7810\n",
      "Epoch [1/10], step [4400,6000], Loss: 0.0068\n",
      "Epoch [1/10], step [4500,6000], Loss: 0.3820\n",
      "Epoch [1/10], step [4600,6000], Loss: 0.1074\n",
      "Epoch [1/10], step [4700,6000], Loss: 0.0008\n",
      "Epoch [1/10], step [4800,6000], Loss: 0.0029\n",
      "Epoch [1/10], step [4900,6000], Loss: 0.0808\n",
      "Epoch [1/10], step [5000,6000], Loss: 0.1186\n",
      "Epoch [1/10], step [5100,6000], Loss: 0.0226\n",
      "Epoch [1/10], step [5200,6000], Loss: 0.0099\n",
      "Epoch [1/10], step [5300,6000], Loss: 0.0064\n",
      "Epoch [1/10], step [5400,6000], Loss: 0.2438\n",
      "Epoch [1/10], step [5500,6000], Loss: 0.0101\n",
      "Epoch [1/10], step [5600,6000], Loss: 0.1027\n",
      "Epoch [1/10], step [5700,6000], Loss: 0.0235\n",
      "Epoch [1/10], step [5800,6000], Loss: 0.0044\n",
      "Epoch [1/10], step [5900,6000], Loss: 0.0247\n",
      "Epoch [1/10], step [6000,6000], Loss: 0.0277\n",
      "Epoch [2/10], step [100,6000], Loss: 0.1245\n",
      "Epoch [2/10], step [200,6000], Loss: 0.0792\n",
      "Epoch [2/10], step [300,6000], Loss: 0.1725\n",
      "Epoch [2/10], step [400,6000], Loss: 0.0073\n",
      "Epoch [2/10], step [500,6000], Loss: 0.0476\n",
      "Epoch [2/10], step [600,6000], Loss: 0.0425\n",
      "Epoch [2/10], step [700,6000], Loss: 0.0013\n",
      "Epoch [2/10], step [800,6000], Loss: 0.0028\n",
      "Epoch [2/10], step [900,6000], Loss: 0.0004\n",
      "Epoch [2/10], step [1000,6000], Loss: 0.0144\n",
      "Epoch [2/10], step [1100,6000], Loss: 0.1142\n",
      "Epoch [2/10], step [1200,6000], Loss: 0.0046\n",
      "Epoch [2/10], step [1300,6000], Loss: 0.0097\n",
      "Epoch [2/10], step [1400,6000], Loss: 0.0022\n",
      "Epoch [2/10], step [1500,6000], Loss: 0.0104\n",
      "Epoch [2/10], step [1600,6000], Loss: 0.1705\n",
      "Epoch [2/10], step [1700,6000], Loss: 0.0237\n",
      "Epoch [2/10], step [1800,6000], Loss: 0.0026\n",
      "Epoch [2/10], step [1900,6000], Loss: 0.0677\n",
      "Epoch [2/10], step [2000,6000], Loss: 0.0126\n",
      "Epoch [2/10], step [2100,6000], Loss: 0.0216\n",
      "Epoch [2/10], step [2200,6000], Loss: 0.0640\n",
      "Epoch [2/10], step [2300,6000], Loss: 0.0007\n",
      "Epoch [2/10], step [2400,6000], Loss: 0.1552\n",
      "Epoch [2/10], step [2500,6000], Loss: 0.0109\n",
      "Epoch [2/10], step [2600,6000], Loss: 0.0004\n",
      "Epoch [2/10], step [2700,6000], Loss: 0.0014\n",
      "Epoch [2/10], step [2800,6000], Loss: 0.0004\n",
      "Epoch [2/10], step [2900,6000], Loss: 0.0168\n",
      "Epoch [2/10], step [3000,6000], Loss: 0.0535\n",
      "Epoch [2/10], step [3100,6000], Loss: 0.1613\n",
      "Epoch [2/10], step [3200,6000], Loss: 0.0145\n",
      "Epoch [2/10], step [3300,6000], Loss: 0.0126\n",
      "Epoch [2/10], step [3400,6000], Loss: 0.0022\n",
      "Epoch [2/10], step [3500,6000], Loss: 0.0121\n",
      "Epoch [2/10], step [3600,6000], Loss: 0.0076\n",
      "Epoch [2/10], step [3700,6000], Loss: 0.0099\n",
      "Epoch [2/10], step [3800,6000], Loss: 0.0306\n",
      "Epoch [2/10], step [3900,6000], Loss: 0.0550\n",
      "Epoch [2/10], step [4000,6000], Loss: 0.0138\n",
      "Epoch [2/10], step [4100,6000], Loss: 0.0006\n",
      "Epoch [2/10], step [4200,6000], Loss: 0.0623\n",
      "Epoch [2/10], step [4300,6000], Loss: 0.0070\n",
      "Epoch [2/10], step [4400,6000], Loss: 0.0272\n",
      "Epoch [2/10], step [4500,6000], Loss: 0.0244\n",
      "Epoch [2/10], step [4600,6000], Loss: 0.0190\n",
      "Epoch [2/10], step [4700,6000], Loss: 0.1665\n",
      "Epoch [2/10], step [4800,6000], Loss: 0.0495\n",
      "Epoch [2/10], step [4900,6000], Loss: 0.0121\n",
      "Epoch [2/10], step [5000,6000], Loss: 0.0027\n",
      "Epoch [2/10], step [5100,6000], Loss: 0.0055\n",
      "Epoch [2/10], step [5200,6000], Loss: 0.2398\n",
      "Epoch [2/10], step [5300,6000], Loss: 0.0180\n",
      "Epoch [2/10], step [5400,6000], Loss: 0.2334\n",
      "Epoch [2/10], step [5500,6000], Loss: 0.0090\n",
      "Epoch [2/10], step [5600,6000], Loss: 0.0313\n",
      "Epoch [2/10], step [5700,6000], Loss: 0.0007\n",
      "Epoch [2/10], step [5800,6000], Loss: 0.0055\n",
      "Epoch [2/10], step [5900,6000], Loss: 0.0048\n",
      "Epoch [2/10], step [6000,6000], Loss: 0.0003\n",
      "Epoch [3/10], step [100,6000], Loss: 0.0003\n",
      "Epoch [3/10], step [200,6000], Loss: 0.0365\n",
      "Epoch [3/10], step [300,6000], Loss: 0.0017\n",
      "Epoch [3/10], step [400,6000], Loss: 0.2059\n",
      "Epoch [3/10], step [500,6000], Loss: 0.0169\n",
      "Epoch [3/10], step [600,6000], Loss: 0.0160\n",
      "Epoch [3/10], step [700,6000], Loss: 0.0046\n",
      "Epoch [3/10], step [800,6000], Loss: 0.0111\n",
      "Epoch [3/10], step [900,6000], Loss: 0.0019\n",
      "Epoch [3/10], step [1000,6000], Loss: 0.0043\n",
      "Epoch [3/10], step [1100,6000], Loss: 0.0494\n",
      "Epoch [3/10], step [1200,6000], Loss: 0.0008\n",
      "Epoch [3/10], step [1300,6000], Loss: 0.0013\n",
      "Epoch [3/10], step [1400,6000], Loss: 0.0867\n",
      "Epoch [3/10], step [1500,6000], Loss: 0.0384\n",
      "Epoch [3/10], step [1600,6000], Loss: 0.0767\n",
      "Epoch [3/10], step [1700,6000], Loss: 0.0698\n",
      "Epoch [3/10], step [1800,6000], Loss: 0.0081\n",
      "Epoch [3/10], step [1900,6000], Loss: 0.0043\n",
      "Epoch [3/10], step [2000,6000], Loss: 0.0010\n",
      "Epoch [3/10], step [2100,6000], Loss: 0.0388\n",
      "Epoch [3/10], step [2200,6000], Loss: 0.0026\n",
      "Epoch [3/10], step [2300,6000], Loss: 0.0055\n",
      "Epoch [3/10], step [2400,6000], Loss: 0.0033\n",
      "Epoch [3/10], step [2500,6000], Loss: 0.0091\n",
      "Epoch [3/10], step [2600,6000], Loss: 0.0076\n",
      "Epoch [3/10], step [2700,6000], Loss: 0.0017\n",
      "Epoch [3/10], step [2800,6000], Loss: 0.0189\n",
      "Epoch [3/10], step [2900,6000], Loss: 0.2680\n",
      "Epoch [3/10], step [3000,6000], Loss: 0.1499\n",
      "Epoch [3/10], step [3100,6000], Loss: 0.3554\n",
      "Epoch [3/10], step [3200,6000], Loss: 0.0123\n",
      "Epoch [3/10], step [3300,6000], Loss: 0.0292\n",
      "Epoch [3/10], step [3400,6000], Loss: 0.2306\n",
      "Epoch [3/10], step [3500,6000], Loss: 0.0091\n",
      "Epoch [3/10], step [3600,6000], Loss: 0.2380\n",
      "Epoch [3/10], step [3700,6000], Loss: 0.0030\n",
      "Epoch [3/10], step [3800,6000], Loss: 0.0015\n",
      "Epoch [3/10], step [3900,6000], Loss: 0.0009\n",
      "Epoch [3/10], step [4000,6000], Loss: 0.0014\n",
      "Epoch [3/10], step [4100,6000], Loss: 0.0034\n",
      "Epoch [3/10], step [4200,6000], Loss: 0.1000\n",
      "Epoch [3/10], step [4300,6000], Loss: 0.0013\n",
      "Epoch [3/10], step [4400,6000], Loss: 0.0039\n",
      "Epoch [3/10], step [4500,6000], Loss: 0.0358\n",
      "Epoch [3/10], step [4600,6000], Loss: 0.0018\n",
      "Epoch [3/10], step [4700,6000], Loss: 0.0087\n",
      "Epoch [3/10], step [4800,6000], Loss: 0.0016\n",
      "Epoch [3/10], step [4900,6000], Loss: 0.0004\n",
      "Epoch [3/10], step [5000,6000], Loss: 0.0658\n",
      "Epoch [3/10], step [5100,6000], Loss: 0.0010\n",
      "Epoch [3/10], step [5200,6000], Loss: 0.0057\n",
      "Epoch [3/10], step [5300,6000], Loss: 0.0026\n",
      "Epoch [3/10], step [5400,6000], Loss: 0.1639\n",
      "Epoch [3/10], step [5500,6000], Loss: 0.0540\n",
      "Epoch [3/10], step [5600,6000], Loss: 0.2566\n",
      "Epoch [3/10], step [5700,6000], Loss: 0.0059\n",
      "Epoch [3/10], step [5800,6000], Loss: 0.0037\n",
      "Epoch [3/10], step [5900,6000], Loss: 0.0026\n",
      "Epoch [3/10], step [6000,6000], Loss: 0.1166\n",
      "Epoch [4/10], step [100,6000], Loss: 0.2523\n",
      "Epoch [4/10], step [200,6000], Loss: 0.1439\n",
      "Epoch [4/10], step [300,6000], Loss: 0.0103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], step [400,6000], Loss: 0.0138\n",
      "Epoch [4/10], step [500,6000], Loss: 0.0047\n",
      "Epoch [4/10], step [600,6000], Loss: 0.0254\n",
      "Epoch [4/10], step [700,6000], Loss: 0.1005\n",
      "Epoch [4/10], step [800,6000], Loss: 0.0019\n",
      "Epoch [4/10], step [900,6000], Loss: 0.0081\n",
      "Epoch [4/10], step [1000,6000], Loss: 0.0019\n",
      "Epoch [4/10], step [1100,6000], Loss: 0.0020\n",
      "Epoch [4/10], step [1200,6000], Loss: 0.7664\n",
      "Epoch [4/10], step [1300,6000], Loss: 0.0422\n",
      "Epoch [4/10], step [1400,6000], Loss: 0.0139\n",
      "Epoch [4/10], step [1500,6000], Loss: 0.3469\n",
      "Epoch [4/10], step [1600,6000], Loss: 0.0884\n",
      "Epoch [4/10], step [1700,6000], Loss: 0.0051\n",
      "Epoch [4/10], step [1800,6000], Loss: 0.0511\n",
      "Epoch [4/10], step [1900,6000], Loss: 0.0268\n",
      "Epoch [4/10], step [2000,6000], Loss: 0.0616\n",
      "Epoch [4/10], step [2100,6000], Loss: 0.0457\n",
      "Epoch [4/10], step [2200,6000], Loss: 0.0014\n",
      "Epoch [4/10], step [2300,6000], Loss: 0.0011\n",
      "Epoch [4/10], step [2400,6000], Loss: 0.0010\n",
      "Epoch [4/10], step [2500,6000], Loss: 0.0375\n",
      "Epoch [4/10], step [2600,6000], Loss: 0.0035\n",
      "Epoch [4/10], step [2700,6000], Loss: 0.0406\n",
      "Epoch [4/10], step [2800,6000], Loss: 0.4605\n",
      "Epoch [4/10], step [2900,6000], Loss: 0.0122\n",
      "Epoch [4/10], step [3000,6000], Loss: 0.0017\n",
      "Epoch [4/10], step [3100,6000], Loss: 0.0063\n",
      "Epoch [4/10], step [3200,6000], Loss: 0.0858\n",
      "Epoch [4/10], step [3300,6000], Loss: 0.0065\n",
      "Epoch [4/10], step [3400,6000], Loss: 0.1060\n",
      "Epoch [4/10], step [3500,6000], Loss: 1.0384\n",
      "Epoch [4/10], step [3600,6000], Loss: 0.0014\n",
      "Epoch [4/10], step [3700,6000], Loss: 0.0041\n",
      "Epoch [4/10], step [3800,6000], Loss: 0.1192\n",
      "Epoch [4/10], step [3900,6000], Loss: 0.0075\n",
      "Epoch [4/10], step [4000,6000], Loss: 0.0010\n",
      "Epoch [4/10], step [4100,6000], Loss: 0.0230\n",
      "Epoch [4/10], step [4200,6000], Loss: 0.0171\n",
      "Epoch [4/10], step [4300,6000], Loss: 0.0023\n",
      "Epoch [4/10], step [4400,6000], Loss: 0.0864\n",
      "Epoch [4/10], step [4500,6000], Loss: 0.0636\n",
      "Epoch [4/10], step [4600,6000], Loss: 0.0611\n",
      "Epoch [4/10], step [4700,6000], Loss: 0.0005\n",
      "Epoch [4/10], step [4800,6000], Loss: 0.0001\n",
      "Epoch [4/10], step [4900,6000], Loss: 0.0009\n",
      "Epoch [4/10], step [5000,6000], Loss: 0.0952\n",
      "Epoch [4/10], step [5100,6000], Loss: 0.4629\n",
      "Epoch [4/10], step [5200,6000], Loss: 0.0125\n",
      "Epoch [4/10], step [5300,6000], Loss: 0.0006\n",
      "Epoch [4/10], step [5400,6000], Loss: 0.0142\n",
      "Epoch [4/10], step [5500,6000], Loss: 0.0089\n",
      "Epoch [4/10], step [5600,6000], Loss: 0.0065\n",
      "Epoch [4/10], step [5700,6000], Loss: 0.0292\n",
      "Epoch [4/10], step [5800,6000], Loss: 0.1457\n",
      "Epoch [4/10], step [5900,6000], Loss: 0.0006\n",
      "Epoch [4/10], step [6000,6000], Loss: 0.1926\n",
      "Epoch [5/10], step [100,6000], Loss: 0.0017\n",
      "Epoch [5/10], step [200,6000], Loss: 0.0004\n",
      "Epoch [5/10], step [300,6000], Loss: 0.0014\n",
      "Epoch [5/10], step [400,6000], Loss: 0.0003\n",
      "Epoch [5/10], step [500,6000], Loss: 0.0437\n",
      "Epoch [5/10], step [600,6000], Loss: 0.3868\n",
      "Epoch [5/10], step [700,6000], Loss: 0.0133\n",
      "Epoch [5/10], step [800,6000], Loss: 0.0001\n",
      "Epoch [5/10], step [900,6000], Loss: 0.0014\n",
      "Epoch [5/10], step [1000,6000], Loss: 0.0007\n",
      "Epoch [5/10], step [1100,6000], Loss: 0.0007\n",
      "Epoch [5/10], step [1200,6000], Loss: 0.0258\n",
      "Epoch [5/10], step [1300,6000], Loss: 0.0003\n",
      "Epoch [5/10], step [1400,6000], Loss: 0.0003\n",
      "Epoch [5/10], step [1500,6000], Loss: 0.0194\n",
      "Epoch [5/10], step [1600,6000], Loss: 0.0001\n",
      "Epoch [5/10], step [1700,6000], Loss: 0.1091\n",
      "Epoch [5/10], step [1800,6000], Loss: 0.0005\n",
      "Epoch [5/10], step [1900,6000], Loss: 0.0049\n",
      "Epoch [5/10], step [2000,6000], Loss: 0.0226\n",
      "Epoch [5/10], step [2100,6000], Loss: 0.0707\n",
      "Epoch [5/10], step [2200,6000], Loss: 0.0386\n",
      "Epoch [5/10], step [2300,6000], Loss: 0.0105\n",
      "Epoch [5/10], step [2400,6000], Loss: 0.0122\n",
      "Epoch [5/10], step [2500,6000], Loss: 0.0450\n",
      "Epoch [5/10], step [2600,6000], Loss: 0.0066\n",
      "Epoch [5/10], step [2700,6000], Loss: 0.0154\n",
      "Epoch [5/10], step [2800,6000], Loss: 0.0007\n",
      "Epoch [5/10], step [2900,6000], Loss: 0.0031\n",
      "Epoch [5/10], step [3000,6000], Loss: 0.1881\n",
      "Epoch [5/10], step [3100,6000], Loss: 0.0222\n",
      "Epoch [5/10], step [3200,6000], Loss: 0.0043\n",
      "Epoch [5/10], step [3300,6000], Loss: 0.0039\n",
      "Epoch [5/10], step [3400,6000], Loss: 0.0010\n",
      "Epoch [5/10], step [3500,6000], Loss: 0.0142\n",
      "Epoch [5/10], step [3600,6000], Loss: 0.0233\n",
      "Epoch [5/10], step [3700,6000], Loss: 0.0921\n",
      "Epoch [5/10], step [3800,6000], Loss: 0.0044\n",
      "Epoch [5/10], step [3900,6000], Loss: 0.0005\n",
      "Epoch [5/10], step [4000,6000], Loss: 0.0018\n",
      "Epoch [5/10], step [4100,6000], Loss: 0.0012\n",
      "Epoch [5/10], step [4200,6000], Loss: 0.0016\n",
      "Epoch [5/10], step [4300,6000], Loss: 0.0086\n",
      "Epoch [5/10], step [4400,6000], Loss: 0.0027\n",
      "Epoch [5/10], step [4500,6000], Loss: 0.0002\n",
      "Epoch [5/10], step [4600,6000], Loss: 0.0216\n",
      "Epoch [5/10], step [4700,6000], Loss: 0.0033\n",
      "Epoch [5/10], step [4800,6000], Loss: 0.0302\n",
      "Epoch [5/10], step [4900,6000], Loss: 0.0051\n",
      "Epoch [5/10], step [5000,6000], Loss: 0.0171\n",
      "Epoch [5/10], step [5100,6000], Loss: 0.0035\n",
      "Epoch [5/10], step [5200,6000], Loss: 0.0270\n",
      "Epoch [5/10], step [5300,6000], Loss: 0.2106\n",
      "Epoch [5/10], step [5400,6000], Loss: 0.0120\n",
      "Epoch [5/10], step [5500,6000], Loss: 0.0006\n",
      "Epoch [5/10], step [5600,6000], Loss: 0.0185\n",
      "Epoch [5/10], step [5700,6000], Loss: 0.0015\n",
      "Epoch [5/10], step [5800,6000], Loss: 0.0007\n",
      "Epoch [5/10], step [5900,6000], Loss: 0.0064\n",
      "Epoch [5/10], step [6000,6000], Loss: 0.0002\n",
      "Epoch [6/10], step [100,6000], Loss: 0.0330\n",
      "Epoch [6/10], step [200,6000], Loss: 0.0231\n",
      "Epoch [6/10], step [300,6000], Loss: 0.0042\n",
      "Epoch [6/10], step [400,6000], Loss: 0.0133\n",
      "Epoch [6/10], step [500,6000], Loss: 0.8560\n",
      "Epoch [6/10], step [600,6000], Loss: 0.0107\n",
      "Epoch [6/10], step [700,6000], Loss: 0.0002\n",
      "Epoch [6/10], step [800,6000], Loss: 0.0115\n",
      "Epoch [6/10], step [900,6000], Loss: 0.2133\n",
      "Epoch [6/10], step [1000,6000], Loss: 0.0001\n",
      "Epoch [6/10], step [1100,6000], Loss: 0.0282\n",
      "Epoch [6/10], step [1200,6000], Loss: 0.0004\n",
      "Epoch [6/10], step [1300,6000], Loss: 0.0045\n",
      "Epoch [6/10], step [1400,6000], Loss: 0.0145\n",
      "Epoch [6/10], step [1500,6000], Loss: 0.0003\n",
      "Epoch [6/10], step [1600,6000], Loss: 0.0070\n",
      "Epoch [6/10], step [1700,6000], Loss: 0.0975\n",
      "Epoch [6/10], step [1800,6000], Loss: 0.0009\n",
      "Epoch [6/10], step [1900,6000], Loss: 0.0074\n",
      "Epoch [6/10], step [2000,6000], Loss: 0.0035\n",
      "Epoch [6/10], step [2100,6000], Loss: 0.0049\n",
      "Epoch [6/10], step [2200,6000], Loss: 0.0006\n",
      "Epoch [6/10], step [2300,6000], Loss: 0.0010\n",
      "Epoch [6/10], step [2400,6000], Loss: 0.0907\n",
      "Epoch [6/10], step [2500,6000], Loss: 0.3941\n",
      "Epoch [6/10], step [2600,6000], Loss: 0.0266\n",
      "Epoch [6/10], step [2700,6000], Loss: 0.0212\n",
      "Epoch [6/10], step [2800,6000], Loss: 0.0082\n",
      "Epoch [6/10], step [2900,6000], Loss: 0.0012\n",
      "Epoch [6/10], step [3000,6000], Loss: 0.0006\n",
      "Epoch [6/10], step [3100,6000], Loss: 0.0021\n",
      "Epoch [6/10], step [3200,6000], Loss: 0.5176\n",
      "Epoch [6/10], step [3300,6000], Loss: 0.0685\n",
      "Epoch [6/10], step [3400,6000], Loss: 0.0118\n",
      "Epoch [6/10], step [3500,6000], Loss: 0.0015\n",
      "Epoch [6/10], step [3600,6000], Loss: 0.0088\n",
      "Epoch [6/10], step [3700,6000], Loss: 0.0012\n",
      "Epoch [6/10], step [3800,6000], Loss: 0.0016\n",
      "Epoch [6/10], step [3900,6000], Loss: 0.0079\n",
      "Epoch [6/10], step [4000,6000], Loss: 0.0037\n",
      "Epoch [6/10], step [4100,6000], Loss: 0.0001\n",
      "Epoch [6/10], step [4200,6000], Loss: 0.0576\n",
      "Epoch [6/10], step [4300,6000], Loss: 0.0003\n",
      "Epoch [6/10], step [4400,6000], Loss: 0.0027\n",
      "Epoch [6/10], step [4500,6000], Loss: 0.0002\n",
      "Epoch [6/10], step [4600,6000], Loss: 0.1679\n",
      "Epoch [6/10], step [4700,6000], Loss: 0.0139\n",
      "Epoch [6/10], step [4800,6000], Loss: 0.0177\n",
      "Epoch [6/10], step [4900,6000], Loss: 0.0469\n",
      "Epoch [6/10], step [5000,6000], Loss: 0.0025\n",
      "Epoch [6/10], step [5100,6000], Loss: 0.0633\n",
      "Epoch [6/10], step [5200,6000], Loss: 0.0983\n",
      "Epoch [6/10], step [5300,6000], Loss: 0.0104\n",
      "Epoch [6/10], step [5400,6000], Loss: 0.0460\n",
      "Epoch [6/10], step [5500,6000], Loss: 0.0020\n",
      "Epoch [6/10], step [5600,6000], Loss: 0.0116\n",
      "Epoch [6/10], step [5700,6000], Loss: 0.0031\n",
      "Epoch [6/10], step [5800,6000], Loss: 0.0004\n",
      "Epoch [6/10], step [5900,6000], Loss: 0.0010\n",
      "Epoch [6/10], step [6000,6000], Loss: 0.0030\n",
      "Epoch [7/10], step [100,6000], Loss: 0.0005\n",
      "Epoch [7/10], step [200,6000], Loss: 0.0003\n",
      "Epoch [7/10], step [300,6000], Loss: 0.0015\n",
      "Epoch [7/10], step [400,6000], Loss: 0.0016\n",
      "Epoch [7/10], step [500,6000], Loss: 0.0098\n",
      "Epoch [7/10], step [600,6000], Loss: 0.0006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], step [700,6000], Loss: 0.0056\n",
      "Epoch [7/10], step [800,6000], Loss: 0.0009\n",
      "Epoch [7/10], step [900,6000], Loss: 0.0006\n",
      "Epoch [7/10], step [1000,6000], Loss: 0.0140\n",
      "Epoch [7/10], step [1100,6000], Loss: 0.0006\n",
      "Epoch [7/10], step [1200,6000], Loss: 0.0050\n",
      "Epoch [7/10], step [1300,6000], Loss: 0.0019\n",
      "Epoch [7/10], step [1400,6000], Loss: 0.0191\n",
      "Epoch [7/10], step [1500,6000], Loss: 0.0771\n",
      "Epoch [7/10], step [1600,6000], Loss: 0.0043\n",
      "Epoch [7/10], step [1700,6000], Loss: 0.1640\n",
      "Epoch [7/10], step [1800,6000], Loss: 0.0003\n",
      "Epoch [7/10], step [1900,6000], Loss: 0.0053\n",
      "Epoch [7/10], step [2000,6000], Loss: 0.0000\n",
      "Epoch [7/10], step [2100,6000], Loss: 0.0223\n",
      "Epoch [7/10], step [2200,6000], Loss: 0.0002\n",
      "Epoch [7/10], step [2300,6000], Loss: 0.0049\n",
      "Epoch [7/10], step [2400,6000], Loss: 0.0005\n",
      "Epoch [7/10], step [2500,6000], Loss: 0.0048\n",
      "Epoch [7/10], step [2600,6000], Loss: 0.0049\n",
      "Epoch [7/10], step [2700,6000], Loss: 0.0019\n",
      "Epoch [7/10], step [2800,6000], Loss: 0.0026\n",
      "Epoch [7/10], step [2900,6000], Loss: 0.0034\n",
      "Epoch [7/10], step [3000,6000], Loss: 0.0006\n",
      "Epoch [7/10], step [3100,6000], Loss: 0.0020\n",
      "Epoch [7/10], step [3200,6000], Loss: 0.0008\n",
      "Epoch [7/10], step [3300,6000], Loss: 0.0015\n",
      "Epoch [7/10], step [3400,6000], Loss: 0.3072\n",
      "Epoch [7/10], step [3500,6000], Loss: 0.0096\n",
      "Epoch [7/10], step [3600,6000], Loss: 0.0031\n",
      "Epoch [7/10], step [3700,6000], Loss: 0.0030\n",
      "Epoch [7/10], step [3800,6000], Loss: 0.0079\n",
      "Epoch [7/10], step [3900,6000], Loss: 0.0249\n",
      "Epoch [7/10], step [4000,6000], Loss: 0.0368\n",
      "Epoch [7/10], step [4100,6000], Loss: 0.2506\n",
      "Epoch [7/10], step [4200,6000], Loss: 0.0068\n",
      "Epoch [7/10], step [4300,6000], Loss: 0.0112\n",
      "Epoch [7/10], step [4400,6000], Loss: 0.0028\n",
      "Epoch [7/10], step [4500,6000], Loss: 0.0005\n",
      "Epoch [7/10], step [4600,6000], Loss: 0.1140\n",
      "Epoch [7/10], step [4700,6000], Loss: 0.0010\n",
      "Epoch [7/10], step [4800,6000], Loss: 0.0014\n",
      "Epoch [7/10], step [4900,6000], Loss: 0.0010\n",
      "Epoch [7/10], step [5000,6000], Loss: 0.0834\n",
      "Epoch [7/10], step [5100,6000], Loss: 0.0344\n",
      "Epoch [7/10], step [5200,6000], Loss: 0.0002\n",
      "Epoch [7/10], step [5300,6000], Loss: 0.0049\n",
      "Epoch [7/10], step [5400,6000], Loss: 0.0001\n",
      "Epoch [7/10], step [5500,6000], Loss: 0.0105\n",
      "Epoch [7/10], step [5600,6000], Loss: 0.3683\n",
      "Epoch [7/10], step [5700,6000], Loss: 0.0009\n",
      "Epoch [7/10], step [5800,6000], Loss: 0.0018\n",
      "Epoch [7/10], step [5900,6000], Loss: 0.0067\n",
      "Epoch [7/10], step [6000,6000], Loss: 0.0924\n",
      "Epoch [8/10], step [100,6000], Loss: 0.0032\n",
      "Epoch [8/10], step [200,6000], Loss: 0.0414\n",
      "Epoch [8/10], step [300,6000], Loss: 0.0008\n",
      "Epoch [8/10], step [400,6000], Loss: 0.0088\n",
      "Epoch [8/10], step [500,6000], Loss: 0.0029\n",
      "Epoch [8/10], step [600,6000], Loss: 0.0002\n",
      "Epoch [8/10], step [700,6000], Loss: 0.0002\n",
      "Epoch [8/10], step [800,6000], Loss: 0.1624\n",
      "Epoch [8/10], step [900,6000], Loss: 0.0335\n",
      "Epoch [8/10], step [1000,6000], Loss: 0.0152\n",
      "Epoch [8/10], step [1100,6000], Loss: 0.0012\n",
      "Epoch [8/10], step [1200,6000], Loss: 0.0321\n",
      "Epoch [8/10], step [1300,6000], Loss: 0.0087\n",
      "Epoch [8/10], step [1400,6000], Loss: 0.0050\n",
      "Epoch [8/10], step [1500,6000], Loss: 0.0066\n",
      "Epoch [8/10], step [1600,6000], Loss: 0.0008\n",
      "Epoch [8/10], step [1700,6000], Loss: 0.0001\n",
      "Epoch [8/10], step [1800,6000], Loss: 0.0894\n",
      "Epoch [8/10], step [1900,6000], Loss: 0.0014\n",
      "Epoch [8/10], step [2000,6000], Loss: 0.0011\n",
      "Epoch [8/10], step [2100,6000], Loss: 0.0010\n",
      "Epoch [8/10], step [2200,6000], Loss: 0.0001\n",
      "Epoch [8/10], step [2300,6000], Loss: 0.0077\n",
      "Epoch [8/10], step [2400,6000], Loss: 0.0020\n",
      "Epoch [8/10], step [2500,6000], Loss: 0.0011\n",
      "Epoch [8/10], step [2600,6000], Loss: 0.0940\n",
      "Epoch [8/10], step [2700,6000], Loss: 0.0012\n",
      "Epoch [8/10], step [2800,6000], Loss: 0.0016\n",
      "Epoch [8/10], step [2900,6000], Loss: 0.0028\n",
      "Epoch [8/10], step [3000,6000], Loss: 0.0374\n",
      "Epoch [8/10], step [3100,6000], Loss: 0.0922\n",
      "Epoch [8/10], step [3200,6000], Loss: 0.0013\n",
      "Epoch [8/10], step [3300,6000], Loss: 0.0077\n",
      "Epoch [8/10], step [3400,6000], Loss: 0.0079\n",
      "Epoch [8/10], step [3500,6000], Loss: 0.0001\n",
      "Epoch [8/10], step [3600,6000], Loss: 0.0634\n",
      "Epoch [8/10], step [3700,6000], Loss: 0.0101\n",
      "Epoch [8/10], step [3800,6000], Loss: 0.0199\n",
      "Epoch [8/10], step [3900,6000], Loss: 0.0216\n",
      "Epoch [8/10], step [4000,6000], Loss: 0.0015\n",
      "Epoch [8/10], step [4100,6000], Loss: 0.0203\n",
      "Epoch [8/10], step [4200,6000], Loss: 0.2212\n",
      "Epoch [8/10], step [4300,6000], Loss: 0.0462\n",
      "Epoch [8/10], step [4400,6000], Loss: 0.0007\n",
      "Epoch [8/10], step [4500,6000], Loss: 0.0141\n",
      "Epoch [8/10], step [4600,6000], Loss: 0.0001\n",
      "Epoch [8/10], step [4700,6000], Loss: 0.0021\n",
      "Epoch [8/10], step [4800,6000], Loss: 0.0001\n",
      "Epoch [8/10], step [4900,6000], Loss: 0.0105\n",
      "Epoch [8/10], step [5000,6000], Loss: 0.0085\n",
      "Epoch [8/10], step [5100,6000], Loss: 0.0793\n",
      "Epoch [8/10], step [5200,6000], Loss: 0.0189\n",
      "Epoch [8/10], step [5300,6000], Loss: 0.0001\n",
      "Epoch [8/10], step [5400,6000], Loss: 0.0615\n",
      "Epoch [8/10], step [5500,6000], Loss: 0.0014\n",
      "Epoch [8/10], step [5600,6000], Loss: 0.0664\n",
      "Epoch [8/10], step [5700,6000], Loss: 0.0054\n",
      "Epoch [8/10], step [5800,6000], Loss: 0.0002\n",
      "Epoch [8/10], step [5900,6000], Loss: 0.0023\n",
      "Epoch [8/10], step [6000,6000], Loss: 0.0009\n",
      "Epoch [9/10], step [100,6000], Loss: 0.0022\n",
      "Epoch [9/10], step [200,6000], Loss: 0.0006\n",
      "Epoch [9/10], step [300,6000], Loss: 0.0175\n",
      "Epoch [9/10], step [400,6000], Loss: 0.0104\n",
      "Epoch [9/10], step [500,6000], Loss: 0.0017\n",
      "Epoch [9/10], step [600,6000], Loss: 0.0006\n",
      "Epoch [9/10], step [700,6000], Loss: 0.0019\n",
      "Epoch [9/10], step [800,6000], Loss: 0.0481\n",
      "Epoch [9/10], step [900,6000], Loss: 0.0107\n",
      "Epoch [9/10], step [1000,6000], Loss: 0.3630\n",
      "Epoch [9/10], step [1100,6000], Loss: 0.0003\n",
      "Epoch [9/10], step [1200,6000], Loss: 0.0001\n",
      "Epoch [9/10], step [1300,6000], Loss: 0.4857\n",
      "Epoch [9/10], step [1400,6000], Loss: 0.1997\n",
      "Epoch [9/10], step [1500,6000], Loss: 0.0001\n",
      "Epoch [9/10], step [1600,6000], Loss: 0.0395\n",
      "Epoch [9/10], step [1700,6000], Loss: 0.0549\n",
      "Epoch [9/10], step [1800,6000], Loss: 0.0173\n",
      "Epoch [9/10], step [1900,6000], Loss: 0.0001\n",
      "Epoch [9/10], step [2000,6000], Loss: 0.0002\n",
      "Epoch [9/10], step [2100,6000], Loss: 0.0032\n",
      "Epoch [9/10], step [2200,6000], Loss: 0.0140\n",
      "Epoch [9/10], step [2300,6000], Loss: 0.0009\n",
      "Epoch [9/10], step [2400,6000], Loss: 0.0002\n",
      "Epoch [9/10], step [2500,6000], Loss: 0.0003\n",
      "Epoch [9/10], step [2600,6000], Loss: 0.0252\n",
      "Epoch [9/10], step [2700,6000], Loss: 0.0010\n",
      "Epoch [9/10], step [2800,6000], Loss: 0.0008\n",
      "Epoch [9/10], step [2900,6000], Loss: 0.0003\n",
      "Epoch [9/10], step [3000,6000], Loss: 0.0003\n",
      "Epoch [9/10], step [3100,6000], Loss: 0.0003\n",
      "Epoch [9/10], step [3200,6000], Loss: 0.0664\n",
      "Epoch [9/10], step [3300,6000], Loss: 0.0764\n",
      "Epoch [9/10], step [3400,6000], Loss: 0.0088\n",
      "Epoch [9/10], step [3500,6000], Loss: 0.0023\n",
      "Epoch [9/10], step [3600,6000], Loss: 0.0005\n",
      "Epoch [9/10], step [3700,6000], Loss: 0.0003\n",
      "Epoch [9/10], step [3800,6000], Loss: 0.0002\n",
      "Epoch [9/10], step [3900,6000], Loss: 0.0224\n",
      "Epoch [9/10], step [4000,6000], Loss: 0.0012\n",
      "Epoch [9/10], step [4100,6000], Loss: 0.0025\n",
      "Epoch [9/10], step [4200,6000], Loss: 0.0066\n",
      "Epoch [9/10], step [4300,6000], Loss: 0.0001\n",
      "Epoch [9/10], step [4400,6000], Loss: 0.0494\n",
      "Epoch [9/10], step [4500,6000], Loss: 0.0002\n",
      "Epoch [9/10], step [4600,6000], Loss: 0.0001\n",
      "Epoch [9/10], step [4700,6000], Loss: 0.0037\n",
      "Epoch [9/10], step [4800,6000], Loss: 0.0005\n",
      "Epoch [9/10], step [4900,6000], Loss: 0.0000\n",
      "Epoch [9/10], step [5000,6000], Loss: 0.0116\n",
      "Epoch [9/10], step [5100,6000], Loss: 0.0001\n",
      "Epoch [9/10], step [5200,6000], Loss: 0.0013\n",
      "Epoch [9/10], step [5300,6000], Loss: 0.0008\n",
      "Epoch [9/10], step [5400,6000], Loss: 0.1249\n",
      "Epoch [9/10], step [5500,6000], Loss: 0.0001\n",
      "Epoch [9/10], step [5600,6000], Loss: 0.0014\n",
      "Epoch [9/10], step [5700,6000], Loss: 0.0009\n",
      "Epoch [9/10], step [5800,6000], Loss: 0.0202\n",
      "Epoch [9/10], step [5900,6000], Loss: 0.0017\n",
      "Epoch [9/10], step [6000,6000], Loss: 0.0024\n",
      "Epoch [10/10], step [100,6000], Loss: 0.0001\n",
      "Epoch [10/10], step [200,6000], Loss: 0.0138\n",
      "Epoch [10/10], step [300,6000], Loss: 0.0049\n",
      "Epoch [10/10], step [400,6000], Loss: 0.1040\n",
      "Epoch [10/10], step [500,6000], Loss: 0.0006\n",
      "Epoch [10/10], step [600,6000], Loss: 0.0549\n",
      "Epoch [10/10], step [700,6000], Loss: 0.0002\n",
      "Epoch [10/10], step [800,6000], Loss: 0.0011\n",
      "Epoch [10/10], step [900,6000], Loss: 0.0003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], step [1000,6000], Loss: 0.0005\n",
      "Epoch [10/10], step [1100,6000], Loss: 0.0120\n",
      "Epoch [10/10], step [1200,6000], Loss: 0.3943\n",
      "Epoch [10/10], step [1300,6000], Loss: 0.0203\n",
      "Epoch [10/10], step [1400,6000], Loss: 0.0356\n",
      "Epoch [10/10], step [1500,6000], Loss: 0.0003\n",
      "Epoch [10/10], step [1600,6000], Loss: 0.0011\n",
      "Epoch [10/10], step [1700,6000], Loss: 0.0008\n",
      "Epoch [10/10], step [1800,6000], Loss: 0.1079\n",
      "Epoch [10/10], step [1900,6000], Loss: 0.0001\n",
      "Epoch [10/10], step [2000,6000], Loss: 0.0015\n",
      "Epoch [10/10], step [2100,6000], Loss: 0.0005\n",
      "Epoch [10/10], step [2200,6000], Loss: 0.1588\n",
      "Epoch [10/10], step [2300,6000], Loss: 0.0000\n",
      "Epoch [10/10], step [2400,6000], Loss: 0.0015\n",
      "Epoch [10/10], step [2500,6000], Loss: 0.0031\n",
      "Epoch [10/10], step [2600,6000], Loss: 0.0046\n",
      "Epoch [10/10], step [2700,6000], Loss: 0.0296\n",
      "Epoch [10/10], step [2800,6000], Loss: 0.0002\n",
      "Epoch [10/10], step [2900,6000], Loss: 0.0003\n",
      "Epoch [10/10], step [3000,6000], Loss: 0.0004\n",
      "Epoch [10/10], step [3100,6000], Loss: 0.0003\n",
      "Epoch [10/10], step [3200,6000], Loss: 0.0007\n",
      "Epoch [10/10], step [3300,6000], Loss: 0.0024\n",
      "Epoch [10/10], step [3400,6000], Loss: 0.0019\n",
      "Epoch [10/10], step [3500,6000], Loss: 0.0018\n",
      "Epoch [10/10], step [3600,6000], Loss: 0.0084\n",
      "Epoch [10/10], step [3700,6000], Loss: 0.0016\n",
      "Epoch [10/10], step [3800,6000], Loss: 0.0002\n",
      "Epoch [10/10], step [3900,6000], Loss: 0.0178\n",
      "Epoch [10/10], step [4000,6000], Loss: 0.0001\n",
      "Epoch [10/10], step [4100,6000], Loss: 0.0050\n",
      "Epoch [10/10], step [4200,6000], Loss: 0.0002\n",
      "Epoch [10/10], step [4300,6000], Loss: 0.0145\n",
      "Epoch [10/10], step [4400,6000], Loss: 0.0000\n",
      "Epoch [10/10], step [4500,6000], Loss: 0.0013\n",
      "Epoch [10/10], step [4600,6000], Loss: 0.0387\n",
      "Epoch [10/10], step [4700,6000], Loss: 0.0054\n",
      "Epoch [10/10], step [4800,6000], Loss: 0.0014\n",
      "Epoch [10/10], step [4900,6000], Loss: 0.0003\n",
      "Epoch [10/10], step [5000,6000], Loss: 0.0004\n",
      "Epoch [10/10], step [5100,6000], Loss: 0.0005\n",
      "Epoch [10/10], step [5200,6000], Loss: 0.0055\n",
      "Epoch [10/10], step [5300,6000], Loss: 0.0002\n",
      "Epoch [10/10], step [5400,6000], Loss: 0.0049\n",
      "Epoch [10/10], step [5500,6000], Loss: 0.0133\n",
      "Epoch [10/10], step [5600,6000], Loss: 0.0125\n",
      "Epoch [10/10], step [5700,6000], Loss: 0.0099\n",
      "Epoch [10/10], step [5800,6000], Loss: 0.0031\n",
      "Epoch [10/10], step [5900,6000], Loss: 0.0353\n",
      "Epoch [10/10], step [6000,6000], Loss: 0.0983\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    for i,(images, labels) in enumerate(train_loader): \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #forward \n",
    "        outputs = model(images) \n",
    "        loss = criterion(outputs, labels) \n",
    "        \n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(i+1)%100==0: \n",
    "            print('Epoch [{}/{}], step [{},{}], Loss: {:.4f}'.format(epoch+1, epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy of model on 100000 images: 97.84 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0 \n",
    "    total = 0 \n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device) \n",
    "        labels = labels.to(device) \n",
    "        outputs = model(images) \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total+=labels.size(0)\n",
    "        correct+=(predicted==labels).sum().item()\n",
    "    print('test accuracy of model on 100000 images: {} %'.format(100*correct/total))\n",
    "    \n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
