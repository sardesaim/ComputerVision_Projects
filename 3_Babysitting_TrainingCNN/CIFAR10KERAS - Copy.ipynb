{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 763 - Computer Vision - Project 03\n",
    "## Babysitting the training of a Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Import libraries</p>\n",
    "Keras Sequential API was used to build the network. The CIFAR datasets was downloaded from keras datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BF8yKHprcr4W"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPool2D, Dropout, Reshape, BatchNormalization\n",
    "from tensorflow.keras import optimizers, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Import and preprocess the data. \n",
    "    Preprocessing scheme was standard scaling by subtracting mean and dividing by the standard deviation. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "#conv to float\n",
    "(X_train, y_train), (X_test, y_test)=(X_train.astype('float64'), y_train), (X_test.astype('float64'), y_test)\n",
    "#class_names for cifar\n",
    "class_name = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "preprocess_flag=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YWzLGmnfddGv"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X_train, X_test):\n",
    "    X_train-=np.mean(X_train,axis=0)\n",
    "    X_test-=np.mean(X_test,axis=0)\n",
    "    X_train/=np.std(X_train,axis=0)\n",
    "    X_test/=np.std(X_test,axis=0)\n",
    "    return X_train, X_test\n",
    "if preprocess_flag==False:\n",
    "    X_train, X_test=preprocess_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape of the image data. \n",
    "iRows,iCols=X_train[0].shape[0],X_train[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W4JqLMAYdkDC"
   },
   "source": [
    "### Build the CNN \n",
    "<p> A basic 3 layer convolutional network is built and babysitting for it's training is done. The model consists of 3 convolutional blocks with relu activations followed by max pooling and then followed by a FCN. </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model hyperparam\n",
    "LEARNING_RATE=1e-4\n",
    "EPOCHS=10\n",
    "L2_REG=0\n",
    "BATCH_SIZE=128\n",
    "DROPOUT=0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IFCl2SqudvNa"
   },
   "outputs": [],
   "source": [
    "def create_model(L2_REG, drop=False, bnorm=False):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3,3), padding= 'same', activation='relu',input_shape = (iRows, iCols,3)))\n",
    "    model.add(MaxPool2D(pool_size=(2,2), strides = 2, padding='same'))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding= 'same', activation='relu',kernel_regularizer=regularizers.l2(L2_REG), bias_regularizer=regularizers.l2(L2_REG)))\n",
    "    model.add(MaxPool2D(pool_size=(2,2), strides = 2, padding='same'))\n",
    "    if drop==True:\n",
    "        model.add(Dropout(DROPOUT))\n",
    "    if bnorm==True:\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding= 'same', activation='relu', kernel_regularizer=regularizers.l2(L2_REG), bias_regularizer=regularizers.l2(L2_REG)))\n",
    "    model.add(MaxPool2D(pool_size=(2,2), strides = 2, padding='same'))\n",
    "    if drop==True:\n",
    "        model.add(Dropout(DROPOUT))\n",
    "    if bnorm==True:\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units = 512, activation='relu', kernel_regularizer=regularizers.l2(L2_REG), bias_regularizer=regularizers.l2(L2_REG)))\n",
    "    model.add(Dense(units=10,activation='softmax'))\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 588
    },
    "colab_type": "code",
    "id": "hO9C8RVWoHUZ",
    "outputId": "ba5c068b-84a6-4bdc-c26c-f1d708c1ebf9"
   },
   "outputs": [],
   "source": [
    "model=create_model(L2_REG)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzVy2YXrobx3"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.SGD(learning_rate=LEARNING_RATE),loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model hyperparam\n",
    "LEARNING_RATE=1e-4\n",
    "EPOCHS=10\n",
    "L2_REG=1e3\n",
    "BATCH_SIZE=128\n",
    "DROPOUT=0.35\n",
    "\n",
    "model=create_model(L2_REG)\n",
    "model.compile(optimizer=optimizers.SGD(learning_rate=LEARNING_RATE),loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=1, verbose=1, validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model hyperparam\n",
    "LEARNING_RATE=1e-3\n",
    "EPOCHS=10\n",
    "L2_REG=0\n",
    "BATCH_SIZE=128\n",
    "DROPOUT=0.35\n",
    "\n",
    "X_tiny=X_train[:20]\n",
    "y_tiny=y_train[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=create_model(L2_REG)\n",
    "model.compile(optimizer=optimizers.SGD(learning_rate=LEARNING_RATE),loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy'])\n",
    "history = model.fit(X_tiny,y_tiny,batch_size=32,epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(history):\n",
    "    plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "    plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE=1e-6\n",
    "EPOCHS=10\n",
    "L2_REG=1e-6\n",
    "BATCH_SIZE=64\n",
    "DROPOUT=0.35\n",
    "\n",
    "model=create_model(L2_REG)\n",
    "model.compile(optimizer=optimizers.SGD(learning_rate=LEARNING_RATE),loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1,  validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE=1e5\n",
    "EPOCHS=10\n",
    "L2_REG=1e-6\n",
    "BATCH_SIZE=64\n",
    "DROPOUT=0.35\n",
    "\n",
    "model=create_model(L2_REG)\n",
    "model.compile(optimizer=optimizers.SGD(learning_rate=LEARNING_RATE),loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1,  validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE=0.6\n",
    "EPOCHS=10\n",
    "L2_REG=1e-6\n",
    "BATCH_SIZE=64\n",
    "DROPOUT=0.35\n",
    "\n",
    "model=create_model(L2_REG)\n",
    "model.compile(optimizer=optimizers.SGD(learning_rate=LEARNING_RATE),loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1,  validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc=[]\n",
    "lrs=[]\n",
    "l2_regs=[]\n",
    "for i in tqdm(range(100)):\n",
    "    lr = 10**np.random.uniform(-3,-6)\n",
    "    l2_reg = 10**np.random.uniform(-5, 1)\n",
    "    model = create_model(l2_reg, 0.35)\n",
    "    model.compile(optimizer=optimizers.SGD(learning_rate=lr),loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy'])\n",
    "    history = model.fit(X_train,y_train,batch_size=BATCH_SIZE,epochs=5, verbose=1,validation_data=(X_val,y_val))\n",
    "    val_acc.append(history.history['val_sparse_categorical_accuracy'][-1])\n",
    "    lrs.append(lr)\n",
    "    l2_regs.append(l2_reg)\n",
    "    print(history.history['val_sparse_categorical_accuracy'][-1], lr, l2_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs=[val_acc,lrs, l2_regs]\n",
    "logs=np.array(logs)\n",
    "# logs=np.sort(logs,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10=np.argsort(logs[0,:])[::-1][:10]\n",
    "top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs[:,top10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc=[]\n",
    "lrs=[]\n",
    "l2_regs=[]\n",
    "for i in tqdm(range(50)):\n",
    "    lr = 10**np.random.uniform(-3,-4)\n",
    "    l2_reg = 10**np.random.uniform(-4, -1)\n",
    "    model = create_model(l2_reg,0.35)\n",
    "    model.compile(optimizer=optimizers.SGD(learning_rate=lr),loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy'])\n",
    "    history = model.fit(X_train,y_train,batch_size=8,epochs=15, verbose=1,validation_data=(X_val,y_val))\n",
    "    val_acc.append(history.history['val_sparse_categorical_accuracy'][-1])\n",
    "    lrs.append(lr)\n",
    "    l2_regs.append(l2_reg)\n",
    "    print(history.history['val_sparse_categorical_accuracy'][-1], lr, l2_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs=[val_acc,lrs, l2_regs]\n",
    "logs=np.array(logs)\n",
    "# logs=np.sort(logs,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top4=np.argsort(logs[0,:])[::-1][:4]\n",
    "top4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs[:,top4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, l2_reg = logs[1,top4[0]],logs[2,top4[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "model = create_model(l2_reg,0.35)\n",
    "model.compile(optimizer=optimizers.SGD(learning_rate=lr),loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy'])\n",
    "history = model.fit(X_train,y_train,batch_size=8,epochs=50, verbose=1,validation_data=(X_val,y_val), callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict_classes(X_test)\n",
    "mat = confusion_matrix(y_test,y_pred)\n",
    "plot_confusion_matrix(mat,figsize=(9,9), show_normed=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NN_competition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
